#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üç´ ETL CACAO - LE PLUS BEAU SITE DU MONDE
Pipeline Data Engineering pour l'analyse des donn√©es de cacao
"""

from flask import Flask, render_template, send_file, jsonify, request
import os
import pandas as pd
import json
from datetime import datetime
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Configuration
app.config['SECRET_KEY'] = 'etl_cacao_secret_key_2024'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max

# Chemins des datasets
DATASETS_PATH = {
    'raw': 'data/raw/cacao_raw.csv',
    'interim': 'data/interim/cacao_interim.csv',
    'clean': 'data/processed/cacao_clean.csv'
}

# ===========================================
# ROUTES PRINCIPALES
# ===========================================

@app.route('/')
def index():
    """Page d'accueil - Dashboard futuriste ETL Cacao"""
    logger.info("üöÄ Acc√®s au dashboard principal")
    return render_template('index.html')

@app.route('/datasets')
def datasets():
    """Page des datasets - Dashboard futuriste"""
    logger.info("üìä Acc√®s √† la page des datasets")
    return render_template('datasets.html')

@app.route('/transformations')
def transformations():
    """Page des transformations - Dashboard futuriste"""
    logger.info("‚öôÔ∏è Acc√®s √† la page des transformations")
    return render_template('transformations.html')


@app.route('/api/datasets')
def get_datasets():
    """API pour r√©cup√©rer les informations des datasets"""
    try:
        datasets_info = {}
        
        for dataset_type, file_path in DATASETS_PATH.items():
            if os.path.exists(file_path):
                # Lire le fichier CSV
                df = pd.read_csv(file_path)
                
                # Calculer la taille du fichier
                file_size = os.path.getsize(file_path)
                size_mb = file_size / (1024 * 1024)
                
                datasets_info[dataset_type] = {
                    'rows': len(df),
                    'columns': len(df.columns),
                    'size': f"{size_mb:.1f} MB",
                    'columns_list': df.columns.tolist(),
                    'last_modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                    'dtypes': df.dtypes.astype(str).to_dict()
                }
            else:
                datasets_info[dataset_type] = {
                    'error': f"Fichier {file_path} non trouv√©",
                    'rows': 0,
                    'columns': 0,
                    'size': '0 MB'
                }
        
        return jsonify({
            'success': True,
            'datasets': datasets_info,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des datasets: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/dataset/<dataset_type>')
def get_dataset_preview(dataset_type):
    """API pour r√©cup√©rer un aper√ßu d'un dataset"""
    try:
        if dataset_type not in DATASETS_PATH:
            return jsonify({'error': 'Type de dataset invalide'}), 400
        
        file_path = DATASETS_PATH[dataset_type]
        
        if not os.path.exists(file_path):
            return jsonify({'error': 'Fichier non trouv√©'}), 404
        
        # Lire le dataset
        df = pd.read_csv(file_path)
        
        # R√©cup√©rer les 10 premi√®res lignes
        preview_data = df.head(10).to_dict('records')
        
        return jsonify({
            'success': True,
            'dataset_type': dataset_type,
            'filename': os.path.basename(file_path),
            'rows': len(df),
            'columns': len(df.columns),
            'columns_list': df.columns.tolist(),
            'preview_data': preview_data,
            'dtypes': df.dtypes.astype(str).to_dict()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration du dataset {dataset_type}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/download/<dataset_type>')
def download_dataset(dataset_type):
    """T√©l√©chargement d'un dataset"""
    try:
        if dataset_type not in DATASETS_PATH:
            return jsonify({'error': 'Type de dataset invalide'}), 400
        
        file_path = DATASETS_PATH[dataset_type]
        
        if not os.path.exists(file_path):
            return jsonify({'error': 'Fichier non trouv√©'}), 404
        
        logger.info(f"üì• T√©l√©chargement du dataset {dataset_type}")
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=f"cacao_{dataset_type}.csv",
            mimetype='text/csv'
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement du dataset {dataset_type}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/pipeline/status')
def get_pipeline_status():
    """API pour r√©cup√©rer le statut du pipeline ETL"""
    try:
        pipeline_status = {
            'extract': {
                'status': 'completed',
                'description': 'Web scraping termin√©',
                'records': 1795,
                'timestamp': '2024-01-15T10:30:00Z'
            },
            'transform': {
                'status': 'completed',
                'description': 'Transformation des donn√©es termin√©e',
                'steps': [
                    'Nettoyage des caract√®res',
                    'Conversion des types',
                    'Uniformisation des pays',
                    'Imputation des valeurs manquantes'
                ],
                'timestamp': '2024-01-15T10:45:00Z'
            },
            'load': {
                'status': 'completed',
                'description': 'Chargement final termin√©',
                'final_records': 1795,
                'duplicates_removed': 0,
                'timestamp': '2024-01-15T11:00:00Z'
            }
        }
        
        return jsonify({
            'success': True,
            'pipeline': pipeline_status,
            'overall_status': 'completed',
            'last_run': '2024-01-15T11:00:00Z'
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration du statut du pipeline: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/transformations')
def get_transformations():
    """API pour r√©cup√©rer les d√©tails des transformations"""
    try:
        transformations = {
            'character_cleaning': {
                'name': 'Nettoyage des Caract√®res',
                'description': 'Suppression des caract√®res de contr√¥le, sp√©ciaux et correction des probl√®mes d\'encodage',
                'steps': [
                    'D√©tection des caract√®res de contr√¥le',
                    'Suppression des caract√®res sp√©ciaux (*, +)',
                    'Correction des probl√®mes d\'encodage (Nave ‚Üí Naive)',
                    'Uniformisation des cha√Ænes de caract√®res'
                ],
                'files_affected': ['Company', 'Origine sp√©cifique du harirot', 'Type de f√®ve'],
                'records_modified': 6
            },
            'type_conversion': {
                'name': 'Conversion des Types',
                'description': 'Transformation des types de donn√©es pour optimiser l\'analyse',
                'steps': [
                    'Suppression du symbole % et conversion en float',
                    'Conversion des dates et REF en int',
                    'Uniformisation des noms de pays',
                    'Validation des types de donn√©es'
                ],
                'files_affected': ['Pourcentage de cacao', 'Date de la revue', 'REF', 'Broad Bean Origin'],
                'records_modified': 1795
            },
            'missing_values': {
                'name': 'Imputation des Valeurs Manquantes',
                'description': 'Gestion intelligente des valeurs manquantes selon leur proportion',
                'steps': [
                    'Analyse des valeurs manquantes par colonne',
                    'Type de f√®ve: 888 valeurs (49.5%) ‚Üí "Unknown"',
                    'Broad Bean Origin: 74 valeurs (4.1%) ‚Üí Mode "Venezuela"',
                    'Validation de la distribution finale'
                ],
                'files_affected': ['Type de f√®ve', 'Broad Bean Origin'],
                'records_modified': 962
            },
            'quality_check': {
                'name': 'Contr√¥le Qualit√© Final',
                'description': 'Validation finale de la qualit√© des donn√©es',
                'steps': [
                    'V√©rification des doublons (0 trouv√©)',
                    'Contr√¥le de la coh√©rence des donn√©es',
                    'Validation des types de donn√©es',
                    'G√©n√©ration du dataset final'
                ],
                'files_affected': 'Toutes les colonnes',
                'records_modified': 1795
            }
        }
        
        return jsonify({
            'success': True,
            'transformations': transformations,
            'total_steps': len(transformations),
            'pipeline_duration': '30 minutes'
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des transformations: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ===========================================
# ROUTES D'ERREUR
# ===========================================

@app.errorhandler(404)
def not_found(error):
    """Page 404 personnalis√©e"""
    return render_template('index.html'), 404

@app.errorhandler(500)
def internal_error(error):
    """Page 500 personnalis√©e"""
    logger.error(f"‚ùå Erreur interne: {error}")
    return jsonify({
        'success': False,
        'error': 'Erreur interne du serveur'
    }), 500

# ===========================================
# FONCTIONS UTILITAIRES
# ===========================================

def check_datasets_exist():
    """V√©rifier que tous les datasets existent"""
    missing_files = []
    
    for dataset_type, file_path in DATASETS_PATH.items():
        if not os.path.exists(file_path):
            missing_files.append(f"{dataset_type}: {file_path}")
    
    if missing_files:
        logger.warning(f"‚ö†Ô∏è Fichiers manquants: {missing_files}")
        return False
    
    logger.info("‚úÖ Tous les datasets sont pr√©sents")
    return True

def get_dataset_stats(file_path):
    """R√©cup√©rer les statistiques d'un dataset"""
    try:
        if not os.path.exists(file_path):
            return None
        
        df = pd.read_csv(file_path)
        file_size = os.path.getsize(file_path)
        
        return {
            'rows': len(df),
            'columns': len(df.columns),
            'size_bytes': file_size,
            'size_mb': file_size / (1024 * 1024),
            'memory_usage': df.memory_usage(deep=True).sum(),
            'dtypes': df.dtypes.astype(str).to_dict(),
            'null_counts': df.isnull().sum().to_dict()
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse du dataset {file_path}: {e}")
        return None

# ===========================================
# INITIALISATION
# ===========================================

def initialize_app():
    """Initialisation de l'application"""
    logger.info("üç´ Initialisation de ETL Cacao...")
    
    # V√©rifier les datasets
    if not check_datasets_exist():
        logger.warning("‚ö†Ô∏è Certains datasets sont manquants")
    
    # Cr√©er les dossiers n√©cessaires
    os.makedirs('data/raw', exist_ok=True)
    os.makedirs('data/interim', exist_ok=True)
    os.makedirs('data/processed', exist_ok=True)
    
    logger.info("‚úÖ ETL Cacao initialis√© avec succ√®s!")

# Initialiser l'application au d√©marrage
initialize_app()

# ===========================================
# LANCEMENT DE L'APPLICATION
# ===========================================

if __name__ == '__main__':
    print("""
    üç´ ETL CACAO - LE PLUS BEAU SITE DU MONDE
    ==========================================
    
    üöÄ D√©marrage du serveur Flask...
    üìä Pipeline Data Engineering pour le cacao
    üåê Site: http://localhost:5000
    
    """)
    
    # V√©rifier les datasets au d√©marrage
    check_datasets_exist()
    
    # Lancer l'application
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=True,
        threaded=True
    )